# -*- coding: utf-8 -*-
"""SciBERT_for_foodsci_sentiment_BertforSeqClass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14WZYL03zadZc4DwPGEoLhgFFxXjfEeQi

Code adapted from [this tutorial](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#11-using-colab-gpu-for-training)

SciBERT repo [here](https://github.com/allenai/scibert)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
from sklearn.model_selection import train_test_split
import torch
import pandas as pd
import numpy as np
import time
import datetime
import random

#Load data and apply polarity rule
from pathlib import Path

data_path = str(Path(__file__).parent / "../Data")
mturk_abstracts = pd.read_csv(data_path + "/mturk_train.csv")

#Polarity rule: If >=2 positive ratings, then label positive
mturk_abstracts['polarity'] = (mturk_abstracts['count_pos'] >= 2).astype(int)

abstracts = mturk_abstracts['inputtext'].tolist()
labels = mturk_abstracts['polarity'].tolist()

assert len(abstracts) == len(labels)

#Load SciBERT tokenizer
from transformers import AutoTokenizer

print("Loading SciBERT tokenizer...")
tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')

#Encode abstracts, set truncation and padding, and return as PyTorch tensors
MAX_LEN = 512 #Longest BERT input length

input_tensors_list = []
attention_tensors_list = []
tokens_list = []

for a in abstracts:
  encoded_dict = tokenizer.encode_plus(a, max_length = MAX_LEN, pad_to_max_length=True,
                                  add_special_tokens=True, return_attention_mask = True,
                                  return_tensors='pt')
  tokenized = tokenizer.tokenize(a)
  input_tensors_list.append(encoded_dict['input_ids'])
  attention_tensors_list.append(encoded_dict['attention_mask'])
  tokens_list.append(tokenized)

#Convert lists into tensors
input_tensor = torch.cat(input_tensors_list, dim=0)
attention_mask_tensor = torch.cat(attention_tensors_list, dim=0)

#Process inputs through model
from transformers import BertModel

model = BertModel.from_pretrained("allenai/scibert_scivocab_uncased")

with torch.no_grad():
    last_hidden_states = model(input_ids = input_tensor, attention_mask = attention_mask_tensor)

features = last_hidden_states[0][:,0,:].numpy()
train_feat, test_feat, train_labels, test_labels = train_test_split(features, labels, random_state = 2020)

#Train and test logistic model
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score

#Grid search for best parameters
parameters = {'C': np.linspace(0.0001, 100, 20)}
grid_search = GridSearchCV(LogisticRegression(), parameters)
grid_search.fit(train_feat, train_labels)

print('best parameters: ', grid_search.best_params_)
print('best scrores: ', grid_search.best_score_)

model = LogisticRegression(random_state = 2020)
model.fit(train_feat, train_labels)

print(model.score(test_feat, test_labels))
